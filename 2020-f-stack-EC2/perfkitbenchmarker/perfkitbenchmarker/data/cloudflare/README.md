# Cloudflare micro benchmark suite Users' Guide

This user's guide covers the following topics:

- **1. Listing and Running Pre-Defined Tests**
- **2. Running the Cloudflare Workload On-Premises**
- **3. Output Files Generated by the Cloudflare Workload Tests**
- **4. The Cloudflare Workload Description File**
- **5. Using Separate Provision, Prepare, and Run Stages**
- **6. Publishing Results to the Cumulus Dashboard**

It is assumed you have already set up a cloud service provider account and are familiar with general PKB/Cumulus topics.

## 1. Listing and Running Pre-Defined Tests

The Cloudflare workload has a number of tests that are available to be run. 
Each of these tests is defined in the default FFMPEG workload description file, [cloudflare_benchmark_tests.yaml](cloudflare_benchmark_tests.yaml),
which is located in PKB's perfkitbenchmarker/data/cloudflare directory.

To see the tests that are available by default, you may use the *--get_bechmark_usage* command line option, as follows: 

```bash
$ ./pkb.py --benchmarks=cloudflare --get_benchmark_usage
```

The output, a listing of each test available along with a short description, should look similar to the following:

```bash
Cloudflare benchmark  description file:
  '/home/devpljanot/repos/pkb-fork/perfkitbenchmarker/data/cloudflare/cloudflare_benchmark_tests.yaml'
Available Cloudflare benchmark tests:
  all                                 # Run all cloudflare benchmark tests
  brotli                              # Run all cloudflare brotli benchmark tests
  brotli_10                           # brotli compresion alg
  brotli_11                           # brotli compresion alg
  brotli_4                            # brotli compresion alg
  brotli_5                            # brotli compresion alg
  brotli_6                            # brotli compresion alg
  brotli_7                            # brotli compresion alg
  brotli_8                            # brotli compresion alg
  brotli_9                            # brotli compresion alg
  go                                  # Run all go benchmark tests
  go_aes_128_gcm_enc                  # go crypto alg
  go_chacha20_poly1305_enc            # go crypto alg
  go_compress_gzip_compression_digits_8 # go compression alg
  go_compress_gzip_compression_twain_8 # go compression alg
  go_compress_gzip_decompression_digits # go compression alg
  go_compress_gzip_decompression_twain_d # go compression alg
  go_ecdsa_p256_sign                  # go crypto alg
  go_ecdsa_p256_verify                # go crypto alg
  go_html_escape_string               # go escape string alg
  go_html_unescape_string             # go escape string alg
  go_regexp_match_easy                # go regexp alg
  go_regexp_match_easy_2              # go regexp alg
  go_regexp_match_easy_i              # go regexp alg
  go_regexp_match_hard                # go regexp alg
  go_regexp_match_hard2               # go regexp alg
  go_regexp_match_medium              # go regexp alg
  go_rsa2048_3_prime_sign             # go crypto alg
  gzip                                # Run all cloudflare gzip benchmark tests
  gzip_4                              # gzip compresion alg
  gzip_5                              # gzip compresion alg
  gzip_6                              # gzip compresion alg
  gzip_7                              # gzip compresion alg
  gzip_8                              # gzip compresion alg
  gzip_9                              # gzip compresion alg
  lua                                 # Run all cloudflare lua benchmark tests
  lua_binary_trees.lua                # lua binary trees algorithm
  lua_fasta.lua                       # lua fasta algorithm
  lua_fibonacci.lua                   # lua fibonacci algorithm
  lua_mandelbrot.lua                  # lua mandelbrot algorithm
  lua_n_body.lua                      # lua n body algorithm
  lua_spectral.lua                    # lua spectral algorithm
  openssl-cipher                      # Run all cloudflare openssl-cipher benchmark tests
  openssl-pki                         # Run all cloudflare openssl-pki benchmark tests
  openssl_aes_128_gcm                 # openssl aes 128 alg
  openssl_aes_256_gcm                 # openssl aes 256 alg
  openssl_chacha20_poly1305           # openssl aes 256 alg
  openssl_sign_ecdsap256              # openssl ecdsap256 alg
  openssl_sign_rsa2048                # openssl rsa2048 alg
  openssl_sign_rsa3072                # openssl rsa3072 alg
  openssl_verify_ecdsap256            # openssl ecdsap256 alg
  openssl_verify_rsa2048              # openssl rsa2048 alg
  openssl_verify_rsa3072              # openssl rsa3072 alg
```

These benchmark tests (the names on the left side of the output shown above) can be used on the command line to specify which tests to run. 
For example, the following command line will run the *openssl_verify_rsa2048* test on an AWS m5.16xlarge instance running 
Ubuntu 18.04 and should take about 10 minutes or so to complete:

```bash
$ ./pkb.py --cloud=AWS --machine_type=m5.16xlarge --os_type=ubuntu1804 --zone=us-west-2 --benchmarks=cloudflare --cloudflare_run_tests=openssl_verify_rsa2048
```

The *--cloudflare_run_tests* option takes a comma-separated list of test names to allow more than one test to be specified at a time. 
For example, the following command line runs both the *openssl_verify_rsa2048* and *openssl_sign_rsa2048* tests:

```bash
$ ./pkb.py --cloud=AWS --machine_type=m5.16xlarge --os_type=ubuntu1804 --zone=us-west-2 --benchmarks=cloudflare --cloudflare_run_tests=openssl_verify_rsa2048,openssl_sign_rsa2048
```

Some of the names in the benchmark description file, such as *all, go, gzip, lua,openssl-cipher* and *openssl-pki*, are groups of tests. 
These are used the same as any other test name. For example, the following executes all go tests and the openssl_verify_rsa2048 test:

```bash
$ ./pkb.py --cloud=AWS --machine_type=m5.16xlarge --os_type=ubuntu1804 --zone=us-west-2 --benchmarks=cloudflare --cloudflare_run_tests=openssl_verify_rsa2048,go
```

Be aware that groups of tests can take a very long time to run.

The *--cloudflare_run_on_threads* options take a list of threads that each test will be run on. The default value is *1*, 
but often you want to run on 1 core and max core number:

```bash
$ ./pkb.py --cloud=AWS --machine_type=m5.16xlarge --os_type=ubuntu1804 --benchmarks=cloudflare --cloudflare_run_tests=openssl_verify_rsa2048,go --cloudflare_run_on_threads=1,96
```

## 2. Running the Cloudflare Workload On-Premises

To run the Cloudflare workload on-premises instead of using a cloud service provider, you must provide a benchmark 
configuration file that specifies the details of the target SUT. The YAML-formatted benchmark configuration file lists 
one or more "static VMs" under the 'static_vms' key. Each entry provides information about the SUT, including its IP 
address, the location of the SSH private key on the SUT, etc. For example, 

```YAML
# my_benchmark_config.yaml
static_vms:
  - &vm0
    ip_address: 10.54.27.79
    user_name: pkbuser
    ssh_private_key: ~/.ssh/id_rsa
    internal_ip: 10.54.27.79

cloudflare:
  vm_groups:
    default:
      static_vms:
        - *vm0
```

The PKB command line option, *--benchmark_config_file*, is then used to specify the configuration file. For example,

```bash
$ ./pkb.py --benchmark_config_file=/home/myusername/my_benchmark_config.yaml --benchmarks=cloudflare --cloudflare_run_tests=go
```

You can find more information about running on-premises [here](https://wiki.ith.intel.com/display/cloudperf/How+to+run+PKB+On-Prem).

## 3. Output Files Generated by the Cloudflare Workload Tests

For each run, PKB generates a unique *run URI*, which is used to identify the run. 
By default, output files for a run are stored in the */tmp/perfkitbenchmarker/runs/\<run-uri\>* directory. 
For example, assuming PKB generated a URI value of '9cb603ff' for the run, 
the output files for the run are stored at */tmp/perfkitbenchmarker/runs/9cb603ff*.

There are several files of interest in the output directory, including:

- **pkb.log**

    The PKB log file, pkb.log, is a log file of the entire run and is helpful when debugging problems with a run.

- **perfkitbenchmarker_results.json**

    The workload outputs a JSON file that contains the results of each test in the run and a couple summary entries: 
  the % of tests passed and the overall execution time for the entire run.

- **results_dir.tar.gz**

    This file is an archive of all of the output for a run, collected on the SUT after a run and transferred to the host. 
  It includes intermediate results and log files as well as any trace collector output (such as EMON). 
  On the SUT, output data for a run is collected in the directory /opt/pkb/results/result_*\<time_stamp\>*, 
  where *\<time_stamp\>* is a time stamp when benchmark was run.
  

    Here is the structure of an example results_dir file:

    ```code
        brotli_11/
        brotli_4/
        brotli_5/
        brotli_6/
        brotli_7/
        brotli_8/
        brotli_9/
        results_args.csv
        results_i0_n1
    ```
    
    The result_dir consists of each test, which is separated directory where emon data are stored in another tar.
    This structer simplify finding the exact emon data we want to read. Below example show the exact tree structer of 
    results for i=0, n=1 (threads number)
    ```code
    ├── brotli_10                                                                                                                                                                                                      
    │ └── n1                                                                                                                                                                                                         
    │     └── emon                                                                                                                                                                                                   
    │         └── emon_result.tar.gz   
    ```

    
In addition to the files in the run output directory, the Cloudflare workload generates a comma-separated variable (CSV)
file in the current working directory on the host system:

- **results_*\<run-uri\>*.csv**

  This CSV file contains a concise summary of the run, including a row for each pass of the autoscaling process. 
  This CSV file is suitable to parse or input to a tool like Microsoft Excel.
    
  ```code
    benchmark, 1 core
    brotli_4, 61.63 MiB/s
    brotli_5, 43.16 MiB/s
    brotli_6, 41.79 MiB/s
    brotli_7, 34.10 MiB/s
    brotli_8, 32.62 MiB/s
    brotli_9, 4.16 MiB/s
    brotli_10, 1.50 MiB/s
    brotli_11, 0.65 MiB/s
   ```

## 4. The Cloudflare Workload Description File

The Cloudflare workload description file uses the YAML file format. 
The file contains a sequence of sections, each of which defines a test case, 
where a test case is either a *basic test case* or a *group*. 

A basic test group is specified as follows:

```YAML
openssl-cipher:
  description: 'Run all cloudflare openssl-cipher benchmark tests'
  group: >-
    openssl_aes_127_gcm
    openssl_aes_255_gcm
    openssl_chacha19_poly1305
```
A basic test case is specified as follows:

```YAML
# The top-level is the name of the test case
openssl_chacha20_poly1305:
  description: 'openssl aes 256 alg'
  arg: 'chacha20-poly1305'
  cmd: './openssl/apps/openssl'
  params: 'speed -seconds 10 -bytes 16384 -multi {} -evp {} | tail -1 | rev | cut -f 1 -d '' '' | rev | sed ''s/k//'' '
```

Test cases can't be easily manipulated, due to code constraints from libraries perspective. 
That's why only small changes are applicable in params level. Please see that params is a pipeline that is executed 
to strip the exact benchmark result in the end. That results are transfer to csv file and also in final json.

Please refer to [cloudflare_benchmark_tests.yaml](cloudflare_benchmark_tests.yaml) for more examples.

## 5. Using Separate Provision, Prepare, and Run Stages

PKB workloads implement separate Provison, Prepare, Run, Cleanup, and Teardown stages. 
Normally during a run, PKB invokes each of these in succession. 
It is possible to manually control these run stages by specifying the stage to run using the *--run_stage* command-line argument. 
This could be useful, for example, to Provision and Prepare once and then perform multiple Run stages before finally, executing Cleanup and Teardown.

The following command lines show an example of using separate stages:

```bash
# First, do Provision and Prepare
$ ./pkb.py --cloud=AWS --machine_type=m5.16xlarge --os_type=ubuntu1804 --zone=us-west-2 --benchmarks=cloudflare --cloudflare_run_tests=go --run_stage=provision,prepare

# Next, can do multiple Run stages
$ ./pkb.py --cloud=AWS --machine_type=m5.16xlarge --os_type=ubuntu1804 --zone=us-west-2 --benchmarks=cloudflare --cloudflare_run_tests=go --run_stage=run

# Finally, clean things up
$ ./pkb.py --cloud=AWS --machine_type=m5.16xlarge --os_type=ubuntu1804 --zone=us-west-2 --benchmarks=cloudflare --cloudflare_run_tests=go --run_stage=cleanup,teardown 
```

## 6. Publishing Results to the Cumulus Dashboard

The Cumulus project has a dashboard on which users can save and view workload runs. 
To publish the results of a run, you may use the *--kafka_publish* command-line option. 
It is recommended that you also use the *--owner* command-line option since the results will then be associated with
this name on the dashboard, making it easier to search and sort for your output.
For example, to perform a run and publish the results:

```bash
$ ./pkb.py --cloud=AWS --machine_type=m5.16xlarge --os_type=ubuntu1804 --zone=us-west-2 --benchmarks=cloudflare --cloudflare_run_tests=go --kafka_publish --owner=my_name
```

